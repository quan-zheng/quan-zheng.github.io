[{"authors":["Quan Zheng","Matthias Zwicker"],"categories":null,"content":" Preprint [PDF (19.4 MB)]\n Supplementary materials [PDF (6.3 MB)]\n Bibtex [bibtex]\n  ","date":1546318800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546318800,"objectID":"f4d60f2ac66082e662204b74d4501a89","permalink":"https://quan-zheng.github.io/publication/impsamplepss19/","publishdate":"2019-01-01T00:00:00-05:00","relpermalink":"/publication/impsamplepss19/","section":"publication","summary":"Importance sampling is one of the most widely used variance reduction strategies in Monte Carlo rendering. In this paper, we propose a novel importance sampling technique that uses a neural network to learn how to sample from a desired density represented by a set of samples. Our approach considers an existing Monte Carlo rendering algorithm as a black box. During a scene-dependent training phase, we learn to generate samples with a desired density in the primary sample space of the rendering algorithm using maximum likelihood estimation. We leverage a recent neural network architecture that was designed to represent real-valued non-volume preserving (Real NVP) transformations in high dimensional spaces. We use Real NVP to non-linearly warp primary sample space and obtain desired densities. In addition, Real NVP efficiently computes the determinant of the Jacobian of the warp, which is required to implement the change of integration variables implied by the warp. A main advantage of our approach is that it is agnostic of underlying light transport effects, and can be combined with an existing rendering technique by treating it as a black box. We show that our approach leads to effective variance reduction in several practical scenarios.","tags":[],"title":"Learning to importance sample in primary sample space","type":"publication"},{"authors":["Yu Liu","Changwen Zheng","Quan Zheng","Hongliang Yuan"],"categories":null,"content":"","date":1514852481,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514852481,"objectID":"8344c7a1989f09362d579ed05a10f8d1","permalink":"https://quan-zheng.github.io/publication/removemcnoise18/","publishdate":"2018-01-01T19:21:21-05:00","relpermalink":"/publication/removemcnoise18/","section":"publication","summary":"","tags":[],"title":"Removing Monte Carlo Noise Using a Sobel Operator and a Guided Image Filter","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng"],"categories":null,"content":"","date":1491019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491019200,"objectID":"c55ff29139cdb0a05231277cff32205a","permalink":"https://quan-zheng.github.io/publication/neurolens16/","publishdate":"2017-04-01T00:00:00-04:00","relpermalink":"/publication/neurolens16/","section":"publication","summary":"Rendering with full lens model can offer images with photorealistic lens effects, but it leads to high computational costs. This paper proposes a novel camera lens model, NeuroLens, to emulate the imaging of real camera lenses through a data‐driven approach. The mapping of image formation in a camera lens is formulated as imaging regression functions (IRFs), which map input rays to output rays. IRFs are approximated with neural networks, which compactly represent the imaging properties and support parallel evaluation on a graphics processing unit (GPU). To effectively represent spatially varying imaging properties of a camera lens, the input space spanned by incident rays is subdivided into multiple subspaces and each subspace is fitted with a separate IRF. To further raise the evaluation accuracy, a set of neural networks is trained for each IRF and the output is calculated as the average output of the set. The effectiveness of the NeuroLens is demonstrated by fitting a wide range of real camera lenses. Experimental results show that it provides higher imaging accuracy in comparison to state‐of‐the‐art camera lens models, while maintaining the high efficiency for processing camera rays.","tags":[],"title":"NeuroLens: data-driven camera lens simulation using neural networks","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng"],"categories":null,"content":"","date":1488344400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488344400,"objectID":"270f5c40936032920bb72daafa8d8608","permalink":"https://quan-zheng.github.io/publication/polylens17/","publishdate":"2017-03-01T00:00:00-05:00","relpermalink":"/publication/polylens17/","section":"publication","summary":"Lens effects are crucial visual elements in the synthetic imagery, but rendering lens effects with complex full lens models is time-consuming. This paper proposes a polynomial regression-based approach for constructing a sparse and accurate polynomial lens model. Terms of a polynomial are built adaptively in a bottom-up approach. Depending on the distribution of aberrations, this approach partitions the light field and builds separate polynomial models for local light fields. A line pupil-based sampling method is presented to accelerate the generation of camera rays. In addition, a new Monte Carlo estimator is derived to support general Monte Carlo rendering. Experiments show that this approach significantly reduces the time cost of constructing a polynomial lens model in comparison to state-of-the-art methods, while achieving high imaging accuracy.","tags":[],"title":"Adaptive sparse polynomial regression for camera lens simulation","type":"publication"},{"authors":["Hongliang Yuan","Changwen Zheng","Quan Zheng","Yu Liu"],"categories":null,"content":"","date":1485994881,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485994881,"objectID":"60abcb5c1753b9838255f5750ee6ff8d","permalink":"https://quan-zheng.github.io/publication/17grapp_adaptiveorderselection/","publishdate":"2017-02-01T19:21:21-05:00","relpermalink":"/publication/17grapp_adaptiveorderselection/","section":"publication","summary":"","tags":[],"title":"Adaptive rendering with adaptive order selection","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng"],"categories":null,"content":"","date":1462148481,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462148481,"objectID":"c46e81677aa0d417cf6e80767b05d1be","permalink":"https://quan-zheng.github.io/publication/16icnc_adaptivelightpathssampling/","publishdate":"2016-05-01T19:21:21-05:00","relpermalink":"/publication/16icnc_adaptivelightpathssampling/","section":"publication","summary":"","tags":[],"title":"Adaptive light paths sampling through full lens model","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng"],"categories":null,"content":"","date":1433131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433131200,"objectID":"d12c16ab329bd5eafd6b5ab799928552","permalink":"https://quan-zheng.github.io/publication/viapt15/","publishdate":"2015-06-01T00:00:00-04:00","relpermalink":"/publication/viapt15/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Visual importance-based adaptive photon tracing","type":"publication"},{"authors":["Ye Cheng","Quan Zheng","Junkai Peng","Pin Lv","Changwen Zheng"],"categories":null,"content":"","date":1430526081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430526081,"objectID":"5c0d56fa187dad641f8c662cb83aac1e","permalink":"https://quan-zheng.github.io/publication/18spie_opticalsimulation/","publishdate":"2015-05-01T19:21:21-05:00","relpermalink":"/publication/18spie_opticalsimulation/","section":"publication","summary":"","tags":[],"title":"Optical simulation based on physically based renderer","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng"],"categories":null,"content":"","date":1427934081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427934081,"objectID":"9dc3c2abb59df5268e6a2a1a7375766f","permalink":"https://quan-zheng.github.io/publication/15icig_photonshooting/","publishdate":"2015-04-01T19:21:21-05:00","relpermalink":"/publication/15icig_photonshooting/","section":"publication","summary":"","tags":[],"title":"Photon shooting with programmable scalar contribution function","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng"],"categories":null,"content":"","date":1417479681,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417479681,"objectID":"9b696a78cac0039a4691a9026f26d3b7","permalink":"https://quan-zheng.github.io/publication/14jcadcg_visualimportance/","publishdate":"2014-12-01T19:21:21-05:00","relpermalink":"/publication/14jcadcg_visualimportance/","section":"publication","summary":"","tags":[],"title":"Adaptive Photon tracing with Visual Importance Guidance","type":"publication"},{"authors":["Quan Zheng","Changwen Zheng","Fukun Wu","Huafei Yin"],"categories":null,"content":"","date":1396398081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396398081,"objectID":"36a7ac97b3f76dcdb48f2e751f3a4ab4","permalink":"https://quan-zheng.github.io/publication/14jcadcg_lensghost/","publishdate":"2014-04-01T19:21:21-05:00","relpermalink":"/publication/14jcadcg_lensghost/","section":"publication","summary":"","tags":[],"title":"Realistic Rendering of Lens Ghost Effects","type":"publication"}]